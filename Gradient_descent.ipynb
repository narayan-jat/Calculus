{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN77t47Om9tUgOXf7aTmzYC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narayan-jat/Calculus/blob/main/Gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ GRADIENT \\quad DESCENT $$\n",
        "This notebook is dedicated for gradient descent method. Gradient descent method is most commonly used technique in machine learning for optimization purposes. Although, differention can be done in three ways Numerically, Symbolically and using Autograd but here the differentiation of functions will be calculated by using autograd (built-in tool for accurate differentiation) for more accurate answers.Gradient descent formula for functions in multiple varibles is as follows:\n",
        "$$θ = θ - α\\cdot ∇f(θ) $$\n",
        "where:\n",
        ">$* \\quad θ$ represents the vector of parameters of model.\n",
        "\n",
        "> $* \\quad α$ is learning rate which controls the size of the step taken in the direction of the negative gradient.\n",
        "\n",
        "> $* \\quad f(θ)$ represents the gradient of the cost function $f$ with respect to $θ$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MOgYl1Hg2Wgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating minima for the function given below by using gradient descent:\n",
        "$$f(x) = sin(x)$$\n",
        "For this function in one variable gradient descent will updated as follows:\n",
        "$$x_{n+1} = x_{n} - \\alpha \\frac{\\partial f}{\\partial x}$$\n",
        "where:\n",
        "\n",
        "\n",
        "> $* \\quad x_{n}$ is starting guessing point on function.\n",
        "\n",
        "Others refer to same meaning as above mentioned."
      ],
      "metadata": {
        "id": "8Y1UzkZ78-_t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLE07XlN2RUz"
      },
      "outputs": [],
      "source": [
        "# Importing required modules.\n",
        "import autograd.numpy as np\n",
        "from autograd import grad as g"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating differentiation of function.\n",
        "def sinFunction(x):\n",
        "    '''\n",
        "    Return: sin function.\n",
        "\n",
        "    Parameter x: It is a argument given to function.\n",
        "    Precondition: It is any argument.\n",
        "    '''\n",
        "    return np.sin(x)\n",
        "gradient = g(sinFunction)"
      ],
      "metadata": {
        "id": "Fb5VJZH5SnS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating minima.\n",
        "x_n = 1.0             # initially randomly choosen point.\n",
        "alpha = 1                  # learnig factor.\n",
        "val = True\n",
        "while val:\n",
        "    if gradient(x_n) > 0:\n",
        "        xN = x_n - alpha * gradient(x_n)\n",
        "        x_n -= alpha\n",
        "    if gradient(x_n) < 0:           # This will stop the iterative process if dradient is negative.\n",
        "        break\n",
        "print(xN/np.pi)         # Giving value of minima in radian."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA090KKbUoTY",
        "outputId": "b8799d55-e718-4924-e6d2-7618df83a8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.49029345166951793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating minima for the function given below using the gradient descent:\n",
        "$$ f(x) = x^2$$"
      ],
      "metadata": {
        "id": "3JgDOtivmAIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining function.\n",
        "def square(x):\n",
        "    '''\n",
        "    Return: x ** 2 function.\n",
        "\n",
        "    Parameter x: It is a argument given to function.\n",
        "    Precondition: It is any argument.\n",
        "    '''\n",
        "    return x ** 2\n",
        "gradient = g(square)"
      ],
      "metadata": {
        "id": "vsXeJkwUmkWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating minima.\n",
        "x_n = 100.0             # initially randomly choosen point.\n",
        "alpha = 0.001                  # learning factor.\n",
        "val = True\n",
        "while val:\n",
        "    if gradient(x_n) > 0:\n",
        "        xN = x_n - alpha * gradient(x_n)\n",
        "        x_n -= alpha\n",
        "    if gradient(x_n) < 0:           # This will stop the iterative process if dradient is negative.\n",
        "        break\n",
        "print(xN)         # Giving value of minima."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUnYxHChmWxw",
        "outputId": "5056eff1-ab84-4ac5-8c63-f466372f3836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009979998867890678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating minima for the function given below using the gradient descent:\n",
        "$$f(x) = x^2 + y^2$$"
      ],
      "metadata": {
        "id": "65hKHSaNnBSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining functions.\n",
        "def function(X):\n",
        "    '''\n",
        "    Return: x ** 2 + y ** 2 function.\n",
        "\n",
        "    Parameter X: It is a argument given to function.\n",
        "    Precondition: It is any argument.\n",
        "    '''\n",
        "    x, y = X\n",
        "    return x ** 2 + y ** 2\n",
        "gradient = g(function)"
      ],
      "metadata": {
        "id": "bGCw-kKFnN-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating minima.\n",
        "x_n = [float(input(f\"enter value {i} start with:\")) for i in range(1, 3)]             # initially randomly choosen point.\n",
        "alpha = 0.001                # learning factor.\n",
        "previous = [0.0, 0.0]   # accumulator to keep previous values track\n",
        "val = True\n",
        "while val:\n",
        "    x_n[0] = x_n[0] - alpha*(gradient(x_n)[0]) # gradient decent for first variable\n",
        "    x_n[1] = x_n[1] - alpha*(gradient(x_n)[1]) # gradient decent for second variable\n",
        "    if abs(gradient(x_n)[0] - gradient(previous)[0])<0.0001 and abs(gradient(x_n)[1] - gradient(previous)[1])<0.0001:\n",
        "        val = False\n",
        "        print(x_n)\n",
        "    previous = x_n.copy()    # avoiding loss of values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hppM_Svznkkn",
        "outputId": "d67c4ea7-2741-46b9-f0fa-73237698bdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter value 1 start with:56.3\n",
            "enter value 2 start with:98.6\n",
            "[2.849429109536804e-05, 4.990296806400212e-05]\n"
          ]
        }
      ]
    }
  ]
}